AI-Powered Document Q&A System (RAG)

An end-to-end Retrieval-Augmented Generation (RAG) system that enables users to ask natural-language questions over PDF documents.
The system retrieves relevant document context using semantic search and generates grounded answers using a local LLM.

Features

ğŸ“‘ PDF document ingestion

âœ‚ï¸ Intelligent text chunking with overlap

ğŸ”¢ Semantic embeddings using Hugging Face

âš¡ Fast similarity search with FAISS

ğŸ§  Retrieval-Augmented Generation (RAG)

ğŸ’¬ Local LLM answer generation (no paid APIs)

ğŸ” Grounded answers with reduced hallucination

ğŸ§© Modular, production-style architecture

Architecture Overview
PDF Document
     â†“
Text Extraction (PyMuPDF)
     â†“
Text Chunking (LangChain)
     â†“
Embeddings (Sentence Transformers)
     â†“
FAISS Vector Store
     â†“
User Query
     â†“
Query Embedding
     â†“
Top-K Semantic Retrieval
     â†“
Context + Question
     â†“
Local LLM (Ollama)
     â†“
Final Answer

What is RAG?

Retrieval-Augmented Generation (RAG) combines:

Retrieval: Fetching relevant document chunks using embeddings

Generation: Producing answers using an LLM grounded in retrieved context

This approach:

Reduces hallucinations

Improves factual accuracy

Scales to large documents

Run the Project
cd backend
python test_rag.py
